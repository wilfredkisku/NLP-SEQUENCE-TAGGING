{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prometeo_neother_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2PPBrrEj8ps",
        "outputId": "c7e11f84-cb64-42be-dfb3-e2f898b5ef86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#load google drive\n",
        "from google.colab import drive\n",
        "drive._mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import the necessary libraries\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "#essential ml libraries\n",
        "import tensorflow as tf \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#class for preparing and cleaning the data\n",
        "class samsungNLPchallange():\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.file_path_train = '/content/drive/MyDrive/models/prometeo/train/'\n",
        "        self.file_path_test = '/content/drive/MyDrive/models/prometeo/'\n",
        "        self.train_x_y = []\n",
        "        self.train_x_y_f = []\n",
        "\n",
        "        self.classes = []\n",
        "        self.words = []\n",
        "\n",
        "        return None\n",
        "\n",
        "    def prepareTestData(self):\n",
        "        f = open(self.file_path_test+'test_public.csv')\n",
        "        csvreader = csv.reader(f)\n",
        "        header = []\n",
        "        rows = []\n",
        "        tokens = []\n",
        "\n",
        "        header = next(csvreader)\n",
        "        for row in csvreader:\n",
        "            rows.append(row)\n",
        "\n",
        "        for sen in rows:\n",
        "            for w in sen[1].split():\n",
        "                tokens.append(w)\n",
        "\n",
        "        print(len(tokens))\n",
        "        return None\n",
        "    \n",
        "    def prepareTrainData(self):\n",
        "        dir_list = os.listdir(self.file_path_train)\n",
        "        dir_list = sorted(dir_list)\n",
        "        \n",
        "        line_f = []\n",
        "        for i in range(len(dir_list)):\n",
        "            with open(self.file_path_train+dir_list[i], 'r') as f:\n",
        "                line = ''\n",
        "                #line_f = []\n",
        "                while True:\n",
        "                    line = f.readline()\n",
        "                    line_s = line.split('\\t')\n",
        "                    line_t = [] \n",
        "                    for l in line_s:\n",
        "                        line_t.append(l.split())\n",
        "                    line_f.append(line_t)  \n",
        "                    if line == '':\n",
        "                        break\n",
        "            \n",
        "                line_f = line_f[:len(line_f)-1]\n",
        "\n",
        "            f.close()\n",
        "\n",
        "        self.train_x_y = line_f\n",
        "        return line_f\n",
        "\n",
        "    def prepareLabels(self):\n",
        "        \n",
        "        unique = []\n",
        "\n",
        "        for val in self.train_x_y:\n",
        "            for v in val[1]:\n",
        "                if v not in unique:\n",
        "                    unique.append(v)\n",
        "        \n",
        "        unique = sorted(unique)\n",
        "        \n",
        "        self.classes = unique\n",
        "\n",
        "        return unique\n",
        "\n",
        "    def prepareWordBag(self):\n",
        "        \n",
        "        unique = []\n",
        "\n",
        "        for val in self.train_x_y:\n",
        "            for v in val[0]:\n",
        "                if v not in unique:\n",
        "                    unique.append(v)\n",
        "        unique = sorted(unique)\n",
        "        self.words = unique\n",
        "        return unique\n",
        "\n",
        "    #main mapper function (word->idx and classes->idx)\n",
        "    def mapping(self):\n",
        "        \n",
        "        t_train_x_y_f = []\n",
        "        \n",
        "        w_n = len(self.words)\n",
        "        c_n = len(self.classes)\n",
        "        \n",
        "        for x in self.train_x_y:\n",
        "            \n",
        "            w_index = []\n",
        "            \n",
        "            for xx in x[0]:\n",
        "                w_index.append(self.words.index(xx))\n",
        "            \n",
        "            c_index = []\n",
        "            \n",
        "            for yy in x[1]:\n",
        "                c_index.append(self.classes.index(yy))\n",
        "            \n",
        "            t_train_x_y_f.append([w_index,c_index])\n",
        "        \n",
        "        self.train_x_y_f = t_train_x_y_f\n",
        "\n",
        "        return t_train_x_y_f\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    OBJ     = samsungNLPchallange()\n",
        "    train   = OBJ.prepareTrainData()\n",
        "    classes = OBJ.prepareLabels()\n",
        "    words   = OBJ.prepareWordBag()\n",
        "    final   = OBJ.mapping()\n"
      ],
      "metadata": {
        "id": "kpHvHlP_l-uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing the test dataset to be released \n",
        "def mappingTest(test_):\n",
        "  t_test_y = []\n",
        "        \n",
        "  for x in test_:        \n",
        "    w_index = []\n",
        "    for xx in x:\n",
        "      try:\n",
        "        w_index.append(words.index(xx))\n",
        "      except:\n",
        "        w_index.append(words.index(' '))\n",
        "        \n",
        "    t_test_y.append(w_index)\n",
        "  return t_test_y\n",
        "\n",
        "file = open('/content/drive/MyDrive/models/prometeo/test_public.csv')\n",
        "csvreader = csv.reader(file)\n",
        "\n",
        "header = []\n",
        "rows = []\n",
        "\n",
        "header = next(csvreader)\n",
        "for row in csvreader:\n",
        "  rows.append(row)\n",
        "\n",
        "test = []\n",
        "\n",
        "for i in rows:\n",
        "  test.append(i[1].split())"
      ],
      "metadata": {
        "id": "5G5dak_VCNcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### At this moment we have the following data prepared:\n",
        "\n",
        "\n",
        "*   `train` : Contains the supervision data (sequence of words and respective labels).\n",
        "*   `test` : Constains the data to be tested on.\n",
        "*   `classes` : The class labels.\n",
        "*   `words`   : The bag of words that constitutes the words that are accumulated in the dataset.\n",
        "*   `final`   : It is the numeric encoding of the words and labels in the training set.\n",
        "\n"
      ],
      "metadata": {
        "id": "jk-sT9OxDH2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(words))\n",
        "words.append(' ')\n",
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdtgZAVnDGft",
        "outputId": "c89d30e1-3943-4d30-f3e1-91ae9217bf9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1769\n",
            "1770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(classes))\n",
        "classes.append('')\n",
        "print(len(classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j53tzY2aEmcq",
        "outputId": "aef92fd4-2c75-4b69-dada-87c97ace1ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33\n",
            "34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a format that can be used in the tensorflow model\n",
        "new_train = []\n",
        "for i in train:\n",
        "  new_lst = []\n",
        "  for j in range(len(i[0])):\n",
        "    new_lst.append([i[0][j],i[1][j]])\n",
        "  new_train.append(new_lst)"
      ],
      "metadata": {
        "id": "KzlUSrK4olxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = []\n",
        "y = []\n",
        "for lst in final:\n",
        "  X.append(np.array(lst[0], dtype=np.int32))\n",
        "  y.append(np.array(lst[1], dtype=np.int32))"
      ],
      "metadata": {
        "id": "DePH7nOOr8CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 20\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=len(words)-1)\n",
        "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=classes.index('o'))"
      ],
      "metadata": {
        "id": "JYsavVMHtQKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#obtain the dataset that is to be used for training the model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = [to_categorical(i, num_classes=len(classes)) for i in y]\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)"
      ],
      "metadata": {
        "id": "Hx8JjWrSt0l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional"
      ],
      "metadata": {
        "id": "6kEV8ANbumXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = Input(shape=(max_len,))\n",
        "model = Embedding(input_dim=len(words), output_dim=max_len, input_length=max_len)(input)  # 20 embeddings\n",
        "model = Dropout(0.5)(model)\n",
        "model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.5))(model)\n",
        "#model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.5))(model)  # variational biLSTM\n",
        "out = TimeDistributed(Dense(len(classes), activation=\"softmax\"))(model)  # softmax output layer\n",
        "\n",
        "model = Model(input, out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylHL5CP_uro6",
        "outputId": "8d5b51e0-cc70-498e-95c8-b13ae45391b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "#assiting code for finding the recall measure, precision measure and finally the `f1 measure`\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "metadata": {
        "id": "qh5oUa-BH_d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\",f1_m])\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint('model_best.h5', verbose=1, save_best_only=True)\n",
        "history = model.fit(X_tr, np.array(y_tr), batch_size=32, epochs=250, validation_split=0.1, verbose=1,callbacks=[checkpointer])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfGd6j-Uz9p_",
        "outputId": "02164fa6-2fb0-49d2-e808-4eb86e56eb5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 1.1564 - accuracy: 0.8314 - f1_m: 0.7006\n",
            "Epoch 1: val_loss improved from inf to 0.58722, saving model to model_best.h5\n",
            "35/35 [==============================] - 23s 300ms/step - loss: 1.1564 - accuracy: 0.8314 - f1_m: 0.7006 - val_loss: 0.5872 - val_accuracy: 0.8820 - val_f1_m: 0.8818\n",
            "Epoch 2/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.6712 - accuracy: 0.8559 - f1_m: 0.8561\n",
            "Epoch 2: val_loss improved from 0.58722 to 0.56935, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 273ms/step - loss: 0.6712 - accuracy: 0.8559 - f1_m: 0.8561 - val_loss: 0.5694 - val_accuracy: 0.8820 - val_f1_m: 0.8818\n",
            "Epoch 3/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.6450 - accuracy: 0.8559 - f1_m: 0.8554\n",
            "Epoch 3: val_loss improved from 0.56935 to 0.54776, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 282ms/step - loss: 0.6450 - accuracy: 0.8559 - f1_m: 0.8554 - val_loss: 0.5478 - val_accuracy: 0.8820 - val_f1_m: 0.8816\n",
            "Epoch 4/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.6223 - accuracy: 0.8560 - f1_m: 0.8565\n",
            "Epoch 4: val_loss improved from 0.54776 to 0.51844, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 273ms/step - loss: 0.6223 - accuracy: 0.8560 - f1_m: 0.8565 - val_loss: 0.5184 - val_accuracy: 0.8820 - val_f1_m: 0.8822\n",
            "Epoch 5/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.5993 - accuracy: 0.8569 - f1_m: 0.8612\n",
            "Epoch 5: val_loss improved from 0.51844 to 0.51032, saving model to model_best.h5\n",
            "35/35 [==============================] - 9s 271ms/step - loss: 0.5993 - accuracy: 0.8569 - f1_m: 0.8612 - val_loss: 0.5103 - val_accuracy: 0.8828 - val_f1_m: 0.8837\n",
            "Epoch 6/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.5772 - accuracy: 0.8592 - f1_m: 0.8680\n",
            "Epoch 6: val_loss improved from 0.51032 to 0.48525, saving model to model_best.h5\n",
            "35/35 [==============================] - 9s 272ms/step - loss: 0.5772 - accuracy: 0.8592 - f1_m: 0.8680 - val_loss: 0.4852 - val_accuracy: 0.8840 - val_f1_m: 0.8890\n",
            "Epoch 7/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.5482 - accuracy: 0.8608 - f1_m: 0.8782\n",
            "Epoch 7: val_loss improved from 0.48525 to 0.46663, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 273ms/step - loss: 0.5482 - accuracy: 0.8608 - f1_m: 0.8782 - val_loss: 0.4666 - val_accuracy: 0.8844 - val_f1_m: 0.8947\n",
            "Epoch 8/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.5186 - accuracy: 0.8622 - f1_m: 0.8863\n",
            "Epoch 8: val_loss improved from 0.46663 to 0.43965, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 272ms/step - loss: 0.5186 - accuracy: 0.8622 - f1_m: 0.8863 - val_loss: 0.4396 - val_accuracy: 0.8876 - val_f1_m: 0.9011\n",
            "Epoch 9/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.4910 - accuracy: 0.8656 - f1_m: 0.8927\n",
            "Epoch 9: val_loss improved from 0.43965 to 0.43868, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 297ms/step - loss: 0.4910 - accuracy: 0.8656 - f1_m: 0.8927 - val_loss: 0.4387 - val_accuracy: 0.8852 - val_f1_m: 0.9022\n",
            "Epoch 10/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.4746 - accuracy: 0.8676 - f1_m: 0.8965\n",
            "Epoch 10: val_loss improved from 0.43868 to 0.41685, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 272ms/step - loss: 0.4746 - accuracy: 0.8676 - f1_m: 0.8965 - val_loss: 0.4169 - val_accuracy: 0.8884 - val_f1_m: 0.9073\n",
            "Epoch 11/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.4568 - accuracy: 0.8701 - f1_m: 0.8986\n",
            "Epoch 11: val_loss improved from 0.41685 to 0.39950, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 274ms/step - loss: 0.4568 - accuracy: 0.8701 - f1_m: 0.8986 - val_loss: 0.3995 - val_accuracy: 0.8876 - val_f1_m: 0.9114\n",
            "Epoch 12/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.4393 - accuracy: 0.8737 - f1_m: 0.9003\n",
            "Epoch 12: val_loss improved from 0.39950 to 0.39386, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 272ms/step - loss: 0.4393 - accuracy: 0.8737 - f1_m: 0.9003 - val_loss: 0.3939 - val_accuracy: 0.8900 - val_f1_m: 0.9145\n",
            "Epoch 13/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.4317 - accuracy: 0.8765 - f1_m: 0.9014\n",
            "Epoch 13: val_loss did not improve from 0.39386\n",
            "35/35 [==============================] - 10s 272ms/step - loss: 0.4317 - accuracy: 0.8765 - f1_m: 0.9014 - val_loss: 0.3976 - val_accuracy: 0.8860 - val_f1_m: 0.9106\n",
            "Epoch 14/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.4175 - accuracy: 0.8775 - f1_m: 0.9026\n",
            "Epoch 14: val_loss improved from 0.39386 to 0.38203, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 274ms/step - loss: 0.4175 - accuracy: 0.8775 - f1_m: 0.9026 - val_loss: 0.3820 - val_accuracy: 0.8912 - val_f1_m: 0.9139\n",
            "Epoch 15/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.4053 - accuracy: 0.8809 - f1_m: 0.9046\n",
            "Epoch 15: val_loss improved from 0.38203 to 0.38064, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 279ms/step - loss: 0.4053 - accuracy: 0.8809 - f1_m: 0.9046 - val_loss: 0.3806 - val_accuracy: 0.8904 - val_f1_m: 0.9122\n",
            "Epoch 16/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.4008 - accuracy: 0.8838 - f1_m: 0.9047\n",
            "Epoch 16: val_loss improved from 0.38064 to 0.36331, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 271ms/step - loss: 0.4008 - accuracy: 0.8838 - f1_m: 0.9047 - val_loss: 0.3633 - val_accuracy: 0.8964 - val_f1_m: 0.9148\n",
            "Epoch 17/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.3858 - accuracy: 0.8856 - f1_m: 0.9051\n",
            "Epoch 17: val_loss improved from 0.36331 to 0.36293, saving model to model_best.h5\n",
            "35/35 [==============================] - 9s 269ms/step - loss: 0.3858 - accuracy: 0.8856 - f1_m: 0.9051 - val_loss: 0.3629 - val_accuracy: 0.8964 - val_f1_m: 0.9159\n",
            "Epoch 18/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.3792 - accuracy: 0.8897 - f1_m: 0.9062\n",
            "Epoch 18: val_loss improved from 0.36293 to 0.35667, saving model to model_best.h5\n",
            "35/35 [==============================] - 9s 271ms/step - loss: 0.3792 - accuracy: 0.8897 - f1_m: 0.9062 - val_loss: 0.3567 - val_accuracy: 0.8988 - val_f1_m: 0.9155\n",
            "Epoch 19/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.3707 - accuracy: 0.8921 - f1_m: 0.9080\n",
            "Epoch 19: val_loss improved from 0.35667 to 0.35479, saving model to model_best.h5\n",
            "35/35 [==============================] - 12s 332ms/step - loss: 0.3707 - accuracy: 0.8921 - f1_m: 0.9080 - val_loss: 0.3548 - val_accuracy: 0.8988 - val_f1_m: 0.9132\n",
            "Epoch 20/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.3635 - accuracy: 0.8957 - f1_m: 0.9093\n",
            "Epoch 20: val_loss improved from 0.35479 to 0.34276, saving model to model_best.h5\n",
            "35/35 [==============================] - 13s 356ms/step - loss: 0.3635 - accuracy: 0.8957 - f1_m: 0.9093 - val_loss: 0.3428 - val_accuracy: 0.9000 - val_f1_m: 0.9171\n",
            "Epoch 21/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.3564 - accuracy: 0.8970 - f1_m: 0.9104\n",
            "Epoch 21: val_loss did not improve from 0.34276\n",
            "35/35 [==============================] - 10s 275ms/step - loss: 0.3564 - accuracy: 0.8970 - f1_m: 0.9104 - val_loss: 0.3432 - val_accuracy: 0.9016 - val_f1_m: 0.9148\n",
            "Epoch 22/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.3456 - accuracy: 0.9004 - f1_m: 0.9114\n",
            "Epoch 22: val_loss improved from 0.34276 to 0.33739, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 272ms/step - loss: 0.3456 - accuracy: 0.9004 - f1_m: 0.9114 - val_loss: 0.3374 - val_accuracy: 0.9000 - val_f1_m: 0.9177\n",
            "Epoch 23/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.3414 - accuracy: 0.9018 - f1_m: 0.9113\n",
            "Epoch 23: val_loss improved from 0.33739 to 0.33451, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 275ms/step - loss: 0.3414 - accuracy: 0.9018 - f1_m: 0.9113 - val_loss: 0.3345 - val_accuracy: 0.9020 - val_f1_m: 0.9187\n",
            "Epoch 24/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.3338 - accuracy: 0.9020 - f1_m: 0.9124\n",
            "Epoch 24: val_loss did not improve from 0.33451\n",
            "35/35 [==============================] - 10s 283ms/step - loss: 0.3338 - accuracy: 0.9020 - f1_m: 0.9124 - val_loss: 0.3380 - val_accuracy: 0.9044 - val_f1_m: 0.9166\n",
            "Epoch 25/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.3257 - accuracy: 0.9060 - f1_m: 0.9159\n",
            "Epoch 25: val_loss improved from 0.33451 to 0.32289, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 276ms/step - loss: 0.3257 - accuracy: 0.9060 - f1_m: 0.9159 - val_loss: 0.3229 - val_accuracy: 0.9088 - val_f1_m: 0.9184\n",
            "Epoch 26/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.3200 - accuracy: 0.9087 - f1_m: 0.9183\n",
            "Epoch 26: val_loss did not improve from 0.32289\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.3200 - accuracy: 0.9087 - f1_m: 0.9183 - val_loss: 0.3251 - val_accuracy: 0.9024 - val_f1_m: 0.9176\n",
            "Epoch 27/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.3099 - accuracy: 0.9094 - f1_m: 0.9176\n",
            "Epoch 27: val_loss did not improve from 0.32289\n",
            "35/35 [==============================] - 10s 292ms/step - loss: 0.3099 - accuracy: 0.9094 - f1_m: 0.9176 - val_loss: 0.3233 - val_accuracy: 0.9040 - val_f1_m: 0.9172\n",
            "Epoch 28/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.3068 - accuracy: 0.9092 - f1_m: 0.9180\n",
            "Epoch 28: val_loss improved from 0.32289 to 0.31764, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.3068 - accuracy: 0.9092 - f1_m: 0.9180 - val_loss: 0.3176 - val_accuracy: 0.9112 - val_f1_m: 0.9207\n",
            "Epoch 29/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.3005 - accuracy: 0.9123 - f1_m: 0.9206\n",
            "Epoch 29: val_loss improved from 0.31764 to 0.31549, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 288ms/step - loss: 0.3005 - accuracy: 0.9123 - f1_m: 0.9206 - val_loss: 0.3155 - val_accuracy: 0.9104 - val_f1_m: 0.9183\n",
            "Epoch 30/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2983 - accuracy: 0.9134 - f1_m: 0.9205\n",
            "Epoch 30: val_loss improved from 0.31549 to 0.30523, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.2983 - accuracy: 0.9134 - f1_m: 0.9205 - val_loss: 0.3052 - val_accuracy: 0.9132 - val_f1_m: 0.9223\n",
            "Epoch 31/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.9155 - f1_m: 0.9213\n",
            "Epoch 31: val_loss improved from 0.30523 to 0.30435, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 276ms/step - loss: 0.2885 - accuracy: 0.9155 - f1_m: 0.9213 - val_loss: 0.3043 - val_accuracy: 0.9092 - val_f1_m: 0.9216\n",
            "Epoch 32/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2816 - accuracy: 0.9183 - f1_m: 0.9241\n",
            "Epoch 32: val_loss improved from 0.30435 to 0.29880, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 284ms/step - loss: 0.2816 - accuracy: 0.9183 - f1_m: 0.9241 - val_loss: 0.2988 - val_accuracy: 0.9140 - val_f1_m: 0.9236\n",
            "Epoch 33/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2787 - accuracy: 0.9186 - f1_m: 0.9247\n",
            "Epoch 33: val_loss did not improve from 0.29880\n",
            "35/35 [==============================] - 10s 282ms/step - loss: 0.2787 - accuracy: 0.9186 - f1_m: 0.9247 - val_loss: 0.2992 - val_accuracy: 0.9144 - val_f1_m: 0.9230\n",
            "Epoch 34/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2697 - accuracy: 0.9206 - f1_m: 0.9264\n",
            "Epoch 34: val_loss improved from 0.29880 to 0.29097, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 287ms/step - loss: 0.2697 - accuracy: 0.9206 - f1_m: 0.9264 - val_loss: 0.2910 - val_accuracy: 0.9188 - val_f1_m: 0.9261\n",
            "Epoch 35/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.9225 - f1_m: 0.9260\n",
            "Epoch 35: val_loss did not improve from 0.29097\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.2677 - accuracy: 0.9225 - f1_m: 0.9260 - val_loss: 0.2987 - val_accuracy: 0.9132 - val_f1_m: 0.9220\n",
            "Epoch 36/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2609 - accuracy: 0.9226 - f1_m: 0.9275\n",
            "Epoch 36: val_loss improved from 0.29097 to 0.28754, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.2609 - accuracy: 0.9226 - f1_m: 0.9275 - val_loss: 0.2875 - val_accuracy: 0.9172 - val_f1_m: 0.9281\n",
            "Epoch 37/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2537 - accuracy: 0.9262 - f1_m: 0.9304\n",
            "Epoch 37: val_loss improved from 0.28754 to 0.28670, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 284ms/step - loss: 0.2537 - accuracy: 0.9262 - f1_m: 0.9304 - val_loss: 0.2867 - val_accuracy: 0.9168 - val_f1_m: 0.9250\n",
            "Epoch 38/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.9260 - f1_m: 0.9289\n",
            "Epoch 38: val_loss improved from 0.28670 to 0.28398, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 285ms/step - loss: 0.2518 - accuracy: 0.9260 - f1_m: 0.9289 - val_loss: 0.2840 - val_accuracy: 0.9160 - val_f1_m: 0.9265\n",
            "Epoch 39/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2429 - accuracy: 0.9268 - f1_m: 0.9303\n",
            "Epoch 39: val_loss improved from 0.28398 to 0.27774, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 283ms/step - loss: 0.2429 - accuracy: 0.9268 - f1_m: 0.9303 - val_loss: 0.2777 - val_accuracy: 0.9216 - val_f1_m: 0.9297\n",
            "Epoch 40/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2392 - accuracy: 0.9307 - f1_m: 0.9324\n",
            "Epoch 40: val_loss did not improve from 0.27774\n",
            "35/35 [==============================] - 10s 283ms/step - loss: 0.2392 - accuracy: 0.9307 - f1_m: 0.9324 - val_loss: 0.2778 - val_accuracy: 0.9228 - val_f1_m: 0.9306\n",
            "Epoch 41/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2323 - accuracy: 0.9344 - f1_m: 0.9339\n",
            "Epoch 41: val_loss improved from 0.27774 to 0.27357, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 280ms/step - loss: 0.2323 - accuracy: 0.9344 - f1_m: 0.9339 - val_loss: 0.2736 - val_accuracy: 0.9232 - val_f1_m: 0.9294\n",
            "Epoch 42/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2299 - accuracy: 0.9338 - f1_m: 0.9346\n",
            "Epoch 42: val_loss did not improve from 0.27357\n",
            "35/35 [==============================] - 10s 282ms/step - loss: 0.2299 - accuracy: 0.9338 - f1_m: 0.9346 - val_loss: 0.2765 - val_accuracy: 0.9244 - val_f1_m: 0.9302\n",
            "Epoch 43/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2223 - accuracy: 0.9355 - f1_m: 0.9359\n",
            "Epoch 43: val_loss did not improve from 0.27357\n",
            "35/35 [==============================] - 10s 276ms/step - loss: 0.2223 - accuracy: 0.9355 - f1_m: 0.9359 - val_loss: 0.2771 - val_accuracy: 0.9228 - val_f1_m: 0.9275\n",
            "Epoch 44/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2167 - accuracy: 0.9372 - f1_m: 0.9381\n",
            "Epoch 44: val_loss improved from 0.27357 to 0.26610, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.2167 - accuracy: 0.9372 - f1_m: 0.9381 - val_loss: 0.2661 - val_accuracy: 0.9268 - val_f1_m: 0.9314\n",
            "Epoch 45/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2160 - accuracy: 0.9380 - f1_m: 0.9378\n",
            "Epoch 45: val_loss improved from 0.26610 to 0.26376, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.2160 - accuracy: 0.9380 - f1_m: 0.9378 - val_loss: 0.2638 - val_accuracy: 0.9276 - val_f1_m: 0.9319\n",
            "Epoch 46/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2107 - accuracy: 0.9389 - f1_m: 0.9383\n",
            "Epoch 46: val_loss did not improve from 0.26376\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.2107 - accuracy: 0.9389 - f1_m: 0.9383 - val_loss: 0.2742 - val_accuracy: 0.9232 - val_f1_m: 0.9289\n",
            "Epoch 47/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2071 - accuracy: 0.9398 - f1_m: 0.9409\n",
            "Epoch 47: val_loss improved from 0.26376 to 0.26011, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 283ms/step - loss: 0.2071 - accuracy: 0.9398 - f1_m: 0.9409 - val_loss: 0.2601 - val_accuracy: 0.9292 - val_f1_m: 0.9329\n",
            "Epoch 48/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2039 - accuracy: 0.9419 - f1_m: 0.9423\n",
            "Epoch 48: val_loss improved from 0.26011 to 0.25989, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 282ms/step - loss: 0.2039 - accuracy: 0.9419 - f1_m: 0.9423 - val_loss: 0.2599 - val_accuracy: 0.9296 - val_f1_m: 0.9332\n",
            "Epoch 49/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1968 - accuracy: 0.9421 - f1_m: 0.9428\n",
            "Epoch 49: val_loss improved from 0.25989 to 0.25914, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 287ms/step - loss: 0.1968 - accuracy: 0.9421 - f1_m: 0.9428 - val_loss: 0.2591 - val_accuracy: 0.9260 - val_f1_m: 0.9337\n",
            "Epoch 50/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1953 - accuracy: 0.9435 - f1_m: 0.9446\n",
            "Epoch 50: val_loss improved from 0.25914 to 0.25728, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 286ms/step - loss: 0.1953 - accuracy: 0.9435 - f1_m: 0.9446 - val_loss: 0.2573 - val_accuracy: 0.9300 - val_f1_m: 0.9340\n",
            "Epoch 51/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1899 - accuracy: 0.9455 - f1_m: 0.9446\n",
            "Epoch 51: val_loss did not improve from 0.25728\n",
            "35/35 [==============================] - 11s 310ms/step - loss: 0.1899 - accuracy: 0.9455 - f1_m: 0.9446 - val_loss: 0.2602 - val_accuracy: 0.9292 - val_f1_m: 0.9322\n",
            "Epoch 52/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1811 - accuracy: 0.9465 - f1_m: 0.9460\n",
            "Epoch 52: val_loss improved from 0.25728 to 0.25427, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 283ms/step - loss: 0.1811 - accuracy: 0.9465 - f1_m: 0.9460 - val_loss: 0.2543 - val_accuracy: 0.9304 - val_f1_m: 0.9357\n",
            "Epoch 53/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1794 - accuracy: 0.9472 - f1_m: 0.9466\n",
            "Epoch 53: val_loss did not improve from 0.25427\n",
            "35/35 [==============================] - 10s 287ms/step - loss: 0.1794 - accuracy: 0.9472 - f1_m: 0.9466 - val_loss: 0.2587 - val_accuracy: 0.9268 - val_f1_m: 0.9321\n",
            "Epoch 54/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1763 - accuracy: 0.9491 - f1_m: 0.9491\n",
            "Epoch 54: val_loss did not improve from 0.25427\n",
            "35/35 [==============================] - 10s 279ms/step - loss: 0.1763 - accuracy: 0.9491 - f1_m: 0.9491 - val_loss: 0.2608 - val_accuracy: 0.9296 - val_f1_m: 0.9351\n",
            "Epoch 55/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1749 - accuracy: 0.9505 - f1_m: 0.9488\n",
            "Epoch 55: val_loss did not improve from 0.25427\n",
            "35/35 [==============================] - 10s 295ms/step - loss: 0.1749 - accuracy: 0.9505 - f1_m: 0.9488 - val_loss: 0.2543 - val_accuracy: 0.9296 - val_f1_m: 0.9321\n",
            "Epoch 56/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1688 - accuracy: 0.9508 - f1_m: 0.9507\n",
            "Epoch 56: val_loss improved from 0.25427 to 0.25264, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.1688 - accuracy: 0.9508 - f1_m: 0.9507 - val_loss: 0.2526 - val_accuracy: 0.9312 - val_f1_m: 0.9335\n",
            "Epoch 57/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.9509 - f1_m: 0.9512\n",
            "Epoch 57: val_loss did not improve from 0.25264\n",
            "35/35 [==============================] - 10s 283ms/step - loss: 0.1701 - accuracy: 0.9509 - f1_m: 0.9512 - val_loss: 0.2552 - val_accuracy: 0.9308 - val_f1_m: 0.9333\n",
            "Epoch 58/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1648 - accuracy: 0.9519 - f1_m: 0.9520\n",
            "Epoch 58: val_loss did not improve from 0.25264\n",
            "35/35 [==============================] - 10s 280ms/step - loss: 0.1648 - accuracy: 0.9519 - f1_m: 0.9520 - val_loss: 0.2536 - val_accuracy: 0.9344 - val_f1_m: 0.9351\n",
            "Epoch 59/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1690 - accuracy: 0.9526 - f1_m: 0.9521\n",
            "Epoch 59: val_loss improved from 0.25264 to 0.24957, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.1690 - accuracy: 0.9526 - f1_m: 0.9521 - val_loss: 0.2496 - val_accuracy: 0.9336 - val_f1_m: 0.9356\n",
            "Epoch 60/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1548 - accuracy: 0.9550 - f1_m: 0.9541\n",
            "Epoch 60: val_loss did not improve from 0.24957\n",
            "35/35 [==============================] - 10s 280ms/step - loss: 0.1548 - accuracy: 0.9550 - f1_m: 0.9541 - val_loss: 0.2564 - val_accuracy: 0.9324 - val_f1_m: 0.9352\n",
            "Epoch 61/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1555 - accuracy: 0.9548 - f1_m: 0.9543\n",
            "Epoch 61: val_loss did not improve from 0.24957\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.1555 - accuracy: 0.9548 - f1_m: 0.9543 - val_loss: 0.2550 - val_accuracy: 0.9296 - val_f1_m: 0.9344\n",
            "Epoch 62/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1543 - accuracy: 0.9549 - f1_m: 0.9553\n",
            "Epoch 62: val_loss did not improve from 0.24957\n",
            "35/35 [==============================] - 10s 284ms/step - loss: 0.1543 - accuracy: 0.9549 - f1_m: 0.9553 - val_loss: 0.2527 - val_accuracy: 0.9328 - val_f1_m: 0.9349\n",
            "Epoch 63/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1517 - accuracy: 0.9555 - f1_m: 0.9554\n",
            "Epoch 63: val_loss improved from 0.24957 to 0.24602, saving model to model_best.h5\n",
            "35/35 [==============================] - 10s 280ms/step - loss: 0.1517 - accuracy: 0.9555 - f1_m: 0.9554 - val_loss: 0.2460 - val_accuracy: 0.9348 - val_f1_m: 0.9371\n",
            "Epoch 64/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1455 - accuracy: 0.9582 - f1_m: 0.9581\n",
            "Epoch 64: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 285ms/step - loss: 0.1455 - accuracy: 0.9582 - f1_m: 0.9581 - val_loss: 0.2544 - val_accuracy: 0.9324 - val_f1_m: 0.9356\n",
            "Epoch 65/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1478 - accuracy: 0.9560 - f1_m: 0.9560\n",
            "Epoch 65: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 279ms/step - loss: 0.1478 - accuracy: 0.9560 - f1_m: 0.9560 - val_loss: 0.2481 - val_accuracy: 0.9320 - val_f1_m: 0.9358\n",
            "Epoch 66/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1418 - accuracy: 0.9585 - f1_m: 0.9585\n",
            "Epoch 66: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.1418 - accuracy: 0.9585 - f1_m: 0.9585 - val_loss: 0.2507 - val_accuracy: 0.9320 - val_f1_m: 0.9346\n",
            "Epoch 67/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1456 - accuracy: 0.9586 - f1_m: 0.9587\n",
            "Epoch 67: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 276ms/step - loss: 0.1456 - accuracy: 0.9586 - f1_m: 0.9587 - val_loss: 0.2477 - val_accuracy: 0.9324 - val_f1_m: 0.9349\n",
            "Epoch 68/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1405 - accuracy: 0.9588 - f1_m: 0.9589\n",
            "Epoch 68: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.1405 - accuracy: 0.9588 - f1_m: 0.9589 - val_loss: 0.2586 - val_accuracy: 0.9304 - val_f1_m: 0.9337\n",
            "Epoch 69/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1393 - accuracy: 0.9586 - f1_m: 0.9593\n",
            "Epoch 69: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 279ms/step - loss: 0.1393 - accuracy: 0.9586 - f1_m: 0.9593 - val_loss: 0.2577 - val_accuracy: 0.9296 - val_f1_m: 0.9322\n",
            "Epoch 70/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1339 - accuracy: 0.9598 - f1_m: 0.9602\n",
            "Epoch 70: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 280ms/step - loss: 0.1339 - accuracy: 0.9598 - f1_m: 0.9602 - val_loss: 0.2470 - val_accuracy: 0.9312 - val_f1_m: 0.9347\n",
            "Epoch 71/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1345 - accuracy: 0.9601 - f1_m: 0.9604\n",
            "Epoch 71: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 274ms/step - loss: 0.1345 - accuracy: 0.9601 - f1_m: 0.9604 - val_loss: 0.2588 - val_accuracy: 0.9316 - val_f1_m: 0.9328\n",
            "Epoch 72/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9613 - f1_m: 0.9619\n",
            "Epoch 72: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 274ms/step - loss: 0.1307 - accuracy: 0.9613 - f1_m: 0.9619 - val_loss: 0.2568 - val_accuracy: 0.9320 - val_f1_m: 0.9347\n",
            "Epoch 73/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1301 - accuracy: 0.9607 - f1_m: 0.9611\n",
            "Epoch 73: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 279ms/step - loss: 0.1301 - accuracy: 0.9607 - f1_m: 0.9611 - val_loss: 0.2547 - val_accuracy: 0.9316 - val_f1_m: 0.9319\n",
            "Epoch 74/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1290 - accuracy: 0.9610 - f1_m: 0.9613\n",
            "Epoch 74: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.1290 - accuracy: 0.9610 - f1_m: 0.9613 - val_loss: 0.2477 - val_accuracy: 0.9320 - val_f1_m: 0.9335\n",
            "Epoch 75/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 0.9619 - f1_m: 0.9621\n",
            "Epoch 75: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.1267 - accuracy: 0.9619 - f1_m: 0.9621 - val_loss: 0.2570 - val_accuracy: 0.9312 - val_f1_m: 0.9323\n",
            "Epoch 76/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1231 - accuracy: 0.9632 - f1_m: 0.9632\n",
            "Epoch 76: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.1231 - accuracy: 0.9632 - f1_m: 0.9632 - val_loss: 0.2469 - val_accuracy: 0.9324 - val_f1_m: 0.9356\n",
            "Epoch 77/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9640 - f1_m: 0.9638\n",
            "Epoch 77: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.1208 - accuracy: 0.9640 - f1_m: 0.9638 - val_loss: 0.2551 - val_accuracy: 0.9300 - val_f1_m: 0.9349\n",
            "Epoch 78/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1181 - accuracy: 0.9650 - f1_m: 0.9653\n",
            "Epoch 78: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 283ms/step - loss: 0.1181 - accuracy: 0.9650 - f1_m: 0.9653 - val_loss: 0.2616 - val_accuracy: 0.9336 - val_f1_m: 0.9341\n",
            "Epoch 79/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1162 - accuracy: 0.9656 - f1_m: 0.9654\n",
            "Epoch 79: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 282ms/step - loss: 0.1162 - accuracy: 0.9656 - f1_m: 0.9654 - val_loss: 0.2587 - val_accuracy: 0.9340 - val_f1_m: 0.9350\n",
            "Epoch 80/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1199 - accuracy: 0.9624 - f1_m: 0.9634\n",
            "Epoch 80: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 279ms/step - loss: 0.1199 - accuracy: 0.9624 - f1_m: 0.9634 - val_loss: 0.2567 - val_accuracy: 0.9348 - val_f1_m: 0.9350\n",
            "Epoch 81/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1172 - accuracy: 0.9654 - f1_m: 0.9653\n",
            "Epoch 81: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 284ms/step - loss: 0.1172 - accuracy: 0.9654 - f1_m: 0.9653 - val_loss: 0.2635 - val_accuracy: 0.9316 - val_f1_m: 0.9345\n",
            "Epoch 82/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1139 - accuracy: 0.9652 - f1_m: 0.9658\n",
            "Epoch 82: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.1139 - accuracy: 0.9652 - f1_m: 0.9658 - val_loss: 0.2539 - val_accuracy: 0.9308 - val_f1_m: 0.9339\n",
            "Epoch 83/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9678 - f1_m: 0.9674\n",
            "Epoch 83: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 275ms/step - loss: 0.1108 - accuracy: 0.9678 - f1_m: 0.9674 - val_loss: 0.2554 - val_accuracy: 0.9316 - val_f1_m: 0.9331\n",
            "Epoch 84/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.9656 - f1_m: 0.9659\n",
            "Epoch 84: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.1154 - accuracy: 0.9656 - f1_m: 0.9659 - val_loss: 0.2498 - val_accuracy: 0.9348 - val_f1_m: 0.9374\n",
            "Epoch 85/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1098 - accuracy: 0.9669 - f1_m: 0.9675\n",
            "Epoch 85: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 279ms/step - loss: 0.1098 - accuracy: 0.9669 - f1_m: 0.9675 - val_loss: 0.2591 - val_accuracy: 0.9348 - val_f1_m: 0.9362\n",
            "Epoch 86/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9671 - f1_m: 0.9681\n",
            "Epoch 86: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.1114 - accuracy: 0.9671 - f1_m: 0.9681 - val_loss: 0.2582 - val_accuracy: 0.9320 - val_f1_m: 0.9325\n",
            "Epoch 87/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1090 - accuracy: 0.9676 - f1_m: 0.9684\n",
            "Epoch 87: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 274ms/step - loss: 0.1090 - accuracy: 0.9676 - f1_m: 0.9684 - val_loss: 0.2471 - val_accuracy: 0.9352 - val_f1_m: 0.9388\n",
            "Epoch 88/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1079 - accuracy: 0.9674 - f1_m: 0.9680\n",
            "Epoch 88: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.1079 - accuracy: 0.9674 - f1_m: 0.9680 - val_loss: 0.2574 - val_accuracy: 0.9344 - val_f1_m: 0.9365\n",
            "Epoch 89/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1040 - accuracy: 0.9673 - f1_m: 0.9676\n",
            "Epoch 89: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.1040 - accuracy: 0.9673 - f1_m: 0.9676 - val_loss: 0.2503 - val_accuracy: 0.9348 - val_f1_m: 0.9379\n",
            "Epoch 90/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1056 - accuracy: 0.9661 - f1_m: 0.9667\n",
            "Epoch 90: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.1056 - accuracy: 0.9661 - f1_m: 0.9667 - val_loss: 0.2612 - val_accuracy: 0.9332 - val_f1_m: 0.9355\n",
            "Epoch 91/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1054 - accuracy: 0.9679 - f1_m: 0.9687\n",
            "Epoch 91: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 284ms/step - loss: 0.1054 - accuracy: 0.9679 - f1_m: 0.9687 - val_loss: 0.2608 - val_accuracy: 0.9312 - val_f1_m: 0.9330\n",
            "Epoch 92/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9696 - f1_m: 0.9705\n",
            "Epoch 92: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0970 - accuracy: 0.9696 - f1_m: 0.9705 - val_loss: 0.2484 - val_accuracy: 0.9336 - val_f1_m: 0.9367\n",
            "Epoch 93/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1022 - accuracy: 0.9686 - f1_m: 0.9691\n",
            "Epoch 93: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 275ms/step - loss: 0.1022 - accuracy: 0.9686 - f1_m: 0.9691 - val_loss: 0.2506 - val_accuracy: 0.9352 - val_f1_m: 0.9358\n",
            "Epoch 94/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1008 - accuracy: 0.9701 - f1_m: 0.9705\n",
            "Epoch 94: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 282ms/step - loss: 0.1008 - accuracy: 0.9701 - f1_m: 0.9705 - val_loss: 0.2483 - val_accuracy: 0.9348 - val_f1_m: 0.9373\n",
            "Epoch 95/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0946 - accuracy: 0.9718 - f1_m: 0.9722\n",
            "Epoch 95: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 275ms/step - loss: 0.0946 - accuracy: 0.9718 - f1_m: 0.9722 - val_loss: 0.2563 - val_accuracy: 0.9364 - val_f1_m: 0.9373\n",
            "Epoch 96/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 0.9720 - f1_m: 0.9717\n",
            "Epoch 96: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 276ms/step - loss: 0.0940 - accuracy: 0.9720 - f1_m: 0.9717 - val_loss: 0.2628 - val_accuracy: 0.9340 - val_f1_m: 0.9351\n",
            "Epoch 97/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0973 - accuracy: 0.9687 - f1_m: 0.9686\n",
            "Epoch 97: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.0973 - accuracy: 0.9687 - f1_m: 0.9686 - val_loss: 0.2560 - val_accuracy: 0.9352 - val_f1_m: 0.9386\n",
            "Epoch 98/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0933 - accuracy: 0.9716 - f1_m: 0.9718\n",
            "Epoch 98: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 280ms/step - loss: 0.0933 - accuracy: 0.9716 - f1_m: 0.9718 - val_loss: 0.2509 - val_accuracy: 0.9352 - val_f1_m: 0.9375\n",
            "Epoch 99/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9717 - f1_m: 0.9720\n",
            "Epoch 99: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.0918 - accuracy: 0.9717 - f1_m: 0.9720 - val_loss: 0.2516 - val_accuracy: 0.9324 - val_f1_m: 0.9387\n",
            "Epoch 100/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0935 - accuracy: 0.9702 - f1_m: 0.9705\n",
            "Epoch 100: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.0935 - accuracy: 0.9702 - f1_m: 0.9705 - val_loss: 0.2571 - val_accuracy: 0.9360 - val_f1_m: 0.9382\n",
            "Epoch 101/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0901 - accuracy: 0.9715 - f1_m: 0.9719\n",
            "Epoch 101: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.0901 - accuracy: 0.9715 - f1_m: 0.9719 - val_loss: 0.2659 - val_accuracy: 0.9328 - val_f1_m: 0.9367\n",
            "Epoch 102/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9714 - f1_m: 0.9716\n",
            "Epoch 102: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.0916 - accuracy: 0.9714 - f1_m: 0.9716 - val_loss: 0.2703 - val_accuracy: 0.9336 - val_f1_m: 0.9343\n",
            "Epoch 103/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0895 - accuracy: 0.9725 - f1_m: 0.9727\n",
            "Epoch 103: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 283ms/step - loss: 0.0895 - accuracy: 0.9725 - f1_m: 0.9727 - val_loss: 0.2584 - val_accuracy: 0.9360 - val_f1_m: 0.9373\n",
            "Epoch 104/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9739 - f1_m: 0.9738\n",
            "Epoch 104: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 282ms/step - loss: 0.0873 - accuracy: 0.9739 - f1_m: 0.9738 - val_loss: 0.2636 - val_accuracy: 0.9332 - val_f1_m: 0.9356\n",
            "Epoch 105/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0885 - accuracy: 0.9726 - f1_m: 0.9729\n",
            "Epoch 105: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 286ms/step - loss: 0.0885 - accuracy: 0.9726 - f1_m: 0.9729 - val_loss: 0.2568 - val_accuracy: 0.9340 - val_f1_m: 0.9387\n",
            "Epoch 106/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0867 - accuracy: 0.9721 - f1_m: 0.9725\n",
            "Epoch 106: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 289ms/step - loss: 0.0867 - accuracy: 0.9721 - f1_m: 0.9725 - val_loss: 0.2592 - val_accuracy: 0.9356 - val_f1_m: 0.9389\n",
            "Epoch 107/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0864 - accuracy: 0.9729 - f1_m: 0.9735\n",
            "Epoch 107: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 276ms/step - loss: 0.0864 - accuracy: 0.9729 - f1_m: 0.9735 - val_loss: 0.2646 - val_accuracy: 0.9356 - val_f1_m: 0.9356\n",
            "Epoch 108/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0864 - accuracy: 0.9736 - f1_m: 0.9737\n",
            "Epoch 108: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 276ms/step - loss: 0.0864 - accuracy: 0.9736 - f1_m: 0.9737 - val_loss: 0.2607 - val_accuracy: 0.9356 - val_f1_m: 0.9390\n",
            "Epoch 109/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0859 - accuracy: 0.9729 - f1_m: 0.9735\n",
            "Epoch 109: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 286ms/step - loss: 0.0859 - accuracy: 0.9729 - f1_m: 0.9735 - val_loss: 0.2714 - val_accuracy: 0.9368 - val_f1_m: 0.9364\n",
            "Epoch 110/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9726 - f1_m: 0.9731\n",
            "Epoch 110: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 284ms/step - loss: 0.0861 - accuracy: 0.9726 - f1_m: 0.9731 - val_loss: 0.2706 - val_accuracy: 0.9360 - val_f1_m: 0.9375\n",
            "Epoch 111/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9747 - f1_m: 0.9752\n",
            "Epoch 111: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.0819 - accuracy: 0.9747 - f1_m: 0.9752 - val_loss: 0.2690 - val_accuracy: 0.9360 - val_f1_m: 0.9362\n",
            "Epoch 112/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9734 - f1_m: 0.9741\n",
            "Epoch 112: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 279ms/step - loss: 0.0828 - accuracy: 0.9734 - f1_m: 0.9741 - val_loss: 0.2582 - val_accuracy: 0.9364 - val_f1_m: 0.9398\n",
            "Epoch 113/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9754 - f1_m: 0.9758\n",
            "Epoch 113: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 284ms/step - loss: 0.0791 - accuracy: 0.9754 - f1_m: 0.9758 - val_loss: 0.2609 - val_accuracy: 0.9356 - val_f1_m: 0.9370\n",
            "Epoch 114/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9756 - f1_m: 0.9755\n",
            "Epoch 114: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 286ms/step - loss: 0.0781 - accuracy: 0.9756 - f1_m: 0.9755 - val_loss: 0.2737 - val_accuracy: 0.9348 - val_f1_m: 0.9359\n",
            "Epoch 115/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0796 - accuracy: 0.9759 - f1_m: 0.9754\n",
            "Epoch 115: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.0796 - accuracy: 0.9759 - f1_m: 0.9754 - val_loss: 0.2636 - val_accuracy: 0.9356 - val_f1_m: 0.9370\n",
            "Epoch 116/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9761 - f1_m: 0.9758\n",
            "Epoch 116: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 275ms/step - loss: 0.0764 - accuracy: 0.9761 - f1_m: 0.9758 - val_loss: 0.2662 - val_accuracy: 0.9360 - val_f1_m: 0.9376\n",
            "Epoch 117/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9753 - f1_m: 0.9751\n",
            "Epoch 117: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.0785 - accuracy: 0.9753 - f1_m: 0.9751 - val_loss: 0.2740 - val_accuracy: 0.9320 - val_f1_m: 0.9349\n",
            "Epoch 118/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9760 - f1_m: 0.9763\n",
            "Epoch 118: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0748 - accuracy: 0.9760 - f1_m: 0.9763 - val_loss: 0.2705 - val_accuracy: 0.9352 - val_f1_m: 0.9363\n",
            "Epoch 119/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9763 - f1_m: 0.9768\n",
            "Epoch 119: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 280ms/step - loss: 0.0754 - accuracy: 0.9763 - f1_m: 0.9768 - val_loss: 0.2767 - val_accuracy: 0.9340 - val_f1_m: 0.9347\n",
            "Epoch 120/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9766 - f1_m: 0.9773\n",
            "Epoch 120: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0745 - accuracy: 0.9766 - f1_m: 0.9773 - val_loss: 0.2837 - val_accuracy: 0.9348 - val_f1_m: 0.9365\n",
            "Epoch 121/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9769 - f1_m: 0.9770\n",
            "Epoch 121: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0742 - accuracy: 0.9769 - f1_m: 0.9770 - val_loss: 0.2684 - val_accuracy: 0.9356 - val_f1_m: 0.9366\n",
            "Epoch 122/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9772 - f1_m: 0.9775\n",
            "Epoch 122: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 273ms/step - loss: 0.0731 - accuracy: 0.9772 - f1_m: 0.9775 - val_loss: 0.2735 - val_accuracy: 0.9372 - val_f1_m: 0.9366\n",
            "Epoch 123/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9765 - f1_m: 0.9767\n",
            "Epoch 123: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0732 - accuracy: 0.9765 - f1_m: 0.9767 - val_loss: 0.2749 - val_accuracy: 0.9348 - val_f1_m: 0.9350\n",
            "Epoch 124/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9753 - f1_m: 0.9753\n",
            "Epoch 124: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.0749 - accuracy: 0.9753 - f1_m: 0.9753 - val_loss: 0.2650 - val_accuracy: 0.9372 - val_f1_m: 0.9382\n",
            "Epoch 125/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9785 - f1_m: 0.9785\n",
            "Epoch 125: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 280ms/step - loss: 0.0700 - accuracy: 0.9785 - f1_m: 0.9785 - val_loss: 0.2799 - val_accuracy: 0.9336 - val_f1_m: 0.9365\n",
            "Epoch 126/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9775 - f1_m: 0.9775\n",
            "Epoch 126: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 274ms/step - loss: 0.0708 - accuracy: 0.9775 - f1_m: 0.9775 - val_loss: 0.2634 - val_accuracy: 0.9364 - val_f1_m: 0.9375\n",
            "Epoch 127/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9766 - f1_m: 0.9764\n",
            "Epoch 127: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 9s 267ms/step - loss: 0.0729 - accuracy: 0.9766 - f1_m: 0.9764 - val_loss: 0.2757 - val_accuracy: 0.9352 - val_f1_m: 0.9363\n",
            "Epoch 128/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9796 - f1_m: 0.9798\n",
            "Epoch 128: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 274ms/step - loss: 0.0685 - accuracy: 0.9796 - f1_m: 0.9798 - val_loss: 0.2630 - val_accuracy: 0.9376 - val_f1_m: 0.9400\n",
            "Epoch 129/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9782 - f1_m: 0.9784\n",
            "Epoch 129: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 273ms/step - loss: 0.0672 - accuracy: 0.9782 - f1_m: 0.9784 - val_loss: 0.2709 - val_accuracy: 0.9396 - val_f1_m: 0.9393\n",
            "Epoch 130/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9790 - f1_m: 0.9790\n",
            "Epoch 130: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 275ms/step - loss: 0.0675 - accuracy: 0.9790 - f1_m: 0.9790 - val_loss: 0.2794 - val_accuracy: 0.9348 - val_f1_m: 0.9364\n",
            "Epoch 131/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9782 - f1_m: 0.9784\n",
            "Epoch 131: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 9s 269ms/step - loss: 0.0668 - accuracy: 0.9782 - f1_m: 0.9784 - val_loss: 0.2696 - val_accuracy: 0.9368 - val_f1_m: 0.9369\n",
            "Epoch 132/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9788 - f1_m: 0.9785\n",
            "Epoch 132: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 272ms/step - loss: 0.0655 - accuracy: 0.9788 - f1_m: 0.9785 - val_loss: 0.2639 - val_accuracy: 0.9380 - val_f1_m: 0.9399\n",
            "Epoch 133/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9787 - f1_m: 0.9782\n",
            "Epoch 133: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 273ms/step - loss: 0.0699 - accuracy: 0.9787 - f1_m: 0.9782 - val_loss: 0.2718 - val_accuracy: 0.9356 - val_f1_m: 0.9376\n",
            "Epoch 134/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9788 - f1_m: 0.9790\n",
            "Epoch 134: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 9s 268ms/step - loss: 0.0675 - accuracy: 0.9788 - f1_m: 0.9790 - val_loss: 0.2723 - val_accuracy: 0.9360 - val_f1_m: 0.9382\n",
            "Epoch 135/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9799 - f1_m: 0.9801\n",
            "Epoch 135: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 275ms/step - loss: 0.0644 - accuracy: 0.9799 - f1_m: 0.9801 - val_loss: 0.2772 - val_accuracy: 0.9360 - val_f1_m: 0.9378\n",
            "Epoch 136/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9796 - f1_m: 0.9799\n",
            "Epoch 136: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 9s 267ms/step - loss: 0.0631 - accuracy: 0.9796 - f1_m: 0.9799 - val_loss: 0.2760 - val_accuracy: 0.9360 - val_f1_m: 0.9371\n",
            "Epoch 137/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9799 - f1_m: 0.9796\n",
            "Epoch 137: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0612 - accuracy: 0.9799 - f1_m: 0.9796 - val_loss: 0.2781 - val_accuracy: 0.9348 - val_f1_m: 0.9365\n",
            "Epoch 138/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9790 - f1_m: 0.9791\n",
            "Epoch 138: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 272ms/step - loss: 0.0654 - accuracy: 0.9790 - f1_m: 0.9791 - val_loss: 0.2752 - val_accuracy: 0.9344 - val_f1_m: 0.9370\n",
            "Epoch 139/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9797 - f1_m: 0.9797\n",
            "Epoch 139: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 275ms/step - loss: 0.0616 - accuracy: 0.9797 - f1_m: 0.9797 - val_loss: 0.2735 - val_accuracy: 0.9356 - val_f1_m: 0.9381\n",
            "Epoch 140/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9805 - f1_m: 0.9805\n",
            "Epoch 140: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 275ms/step - loss: 0.0618 - accuracy: 0.9805 - f1_m: 0.9805 - val_loss: 0.2772 - val_accuracy: 0.9344 - val_f1_m: 0.9376\n",
            "Epoch 141/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9799 - f1_m: 0.9802\n",
            "Epoch 141: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 273ms/step - loss: 0.0632 - accuracy: 0.9799 - f1_m: 0.9802 - val_loss: 0.2738 - val_accuracy: 0.9368 - val_f1_m: 0.9388\n",
            "Epoch 142/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9791 - f1_m: 0.9795\n",
            "Epoch 142: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 272ms/step - loss: 0.0653 - accuracy: 0.9791 - f1_m: 0.9795 - val_loss: 0.2654 - val_accuracy: 0.9384 - val_f1_m: 0.9390\n",
            "Epoch 143/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9809 - f1_m: 0.9811\n",
            "Epoch 143: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0614 - accuracy: 0.9809 - f1_m: 0.9811 - val_loss: 0.2763 - val_accuracy: 0.9368 - val_f1_m: 0.9382\n",
            "Epoch 144/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9798 - f1_m: 0.9801\n",
            "Epoch 144: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 274ms/step - loss: 0.0617 - accuracy: 0.9798 - f1_m: 0.9801 - val_loss: 0.2703 - val_accuracy: 0.9376 - val_f1_m: 0.9394\n",
            "Epoch 145/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9813 - f1_m: 0.9812\n",
            "Epoch 145: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 9s 269ms/step - loss: 0.0575 - accuracy: 0.9813 - f1_m: 0.9812 - val_loss: 0.2725 - val_accuracy: 0.9372 - val_f1_m: 0.9390\n",
            "Epoch 146/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9802 - f1_m: 0.9805\n",
            "Epoch 146: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 9s 271ms/step - loss: 0.0595 - accuracy: 0.9802 - f1_m: 0.9805 - val_loss: 0.2683 - val_accuracy: 0.9396 - val_f1_m: 0.9411\n",
            "Epoch 147/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9808 - f1_m: 0.9807\n",
            "Epoch 147: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 272ms/step - loss: 0.0573 - accuracy: 0.9808 - f1_m: 0.9807 - val_loss: 0.2799 - val_accuracy: 0.9356 - val_f1_m: 0.9360\n",
            "Epoch 148/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9813 - f1_m: 0.9813\n",
            "Epoch 148: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 274ms/step - loss: 0.0570 - accuracy: 0.9813 - f1_m: 0.9813 - val_loss: 0.2826 - val_accuracy: 0.9360 - val_f1_m: 0.9373\n",
            "Epoch 149/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9796 - f1_m: 0.9796\n",
            "Epoch 149: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.0612 - accuracy: 0.9796 - f1_m: 0.9796 - val_loss: 0.2771 - val_accuracy: 0.9364 - val_f1_m: 0.9361\n",
            "Epoch 150/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.9811 - f1_m: 0.9812\n",
            "Epoch 150: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.0587 - accuracy: 0.9811 - f1_m: 0.9812 - val_loss: 0.2782 - val_accuracy: 0.9392 - val_f1_m: 0.9409\n",
            "Epoch 151/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9815 - f1_m: 0.9817\n",
            "Epoch 151: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 274ms/step - loss: 0.0571 - accuracy: 0.9815 - f1_m: 0.9817 - val_loss: 0.2762 - val_accuracy: 0.9364 - val_f1_m: 0.9377\n",
            "Epoch 152/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9803 - f1_m: 0.9803\n",
            "Epoch 152: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 285ms/step - loss: 0.0590 - accuracy: 0.9803 - f1_m: 0.9803 - val_loss: 0.2773 - val_accuracy: 0.9392 - val_f1_m: 0.9409\n",
            "Epoch 153/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9823 - f1_m: 0.9827\n",
            "Epoch 153: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.0568 - accuracy: 0.9823 - f1_m: 0.9827 - val_loss: 0.2772 - val_accuracy: 0.9384 - val_f1_m: 0.9397\n",
            "Epoch 154/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0600 - accuracy: 0.9803 - f1_m: 0.9808\n",
            "Epoch 154: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0600 - accuracy: 0.9803 - f1_m: 0.9808 - val_loss: 0.2764 - val_accuracy: 0.9360 - val_f1_m: 0.9368\n",
            "Epoch 155/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9823 - f1_m: 0.9819\n",
            "Epoch 155: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 271ms/step - loss: 0.0564 - accuracy: 0.9823 - f1_m: 0.9819 - val_loss: 0.2753 - val_accuracy: 0.9384 - val_f1_m: 0.9391\n",
            "Epoch 156/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9805 - f1_m: 0.9809\n",
            "Epoch 156: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 275ms/step - loss: 0.0570 - accuracy: 0.9805 - f1_m: 0.9809 - val_loss: 0.2787 - val_accuracy: 0.9392 - val_f1_m: 0.9396\n",
            "Epoch 157/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9815 - f1_m: 0.9815\n",
            "Epoch 157: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 9s 269ms/step - loss: 0.0560 - accuracy: 0.9815 - f1_m: 0.9815 - val_loss: 0.2743 - val_accuracy: 0.9384 - val_f1_m: 0.9396\n",
            "Epoch 158/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9823 - f1_m: 0.9825\n",
            "Epoch 158: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 272ms/step - loss: 0.0544 - accuracy: 0.9823 - f1_m: 0.9825 - val_loss: 0.2819 - val_accuracy: 0.9368 - val_f1_m: 0.9388\n",
            "Epoch 159/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9832 - f1_m: 0.9832\n",
            "Epoch 159: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 274ms/step - loss: 0.0536 - accuracy: 0.9832 - f1_m: 0.9832 - val_loss: 0.2725 - val_accuracy: 0.9396 - val_f1_m: 0.9406\n",
            "Epoch 160/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.9823 - f1_m: 0.9823\n",
            "Epoch 160: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 9s 270ms/step - loss: 0.0543 - accuracy: 0.9823 - f1_m: 0.9823 - val_loss: 0.2712 - val_accuracy: 0.9380 - val_f1_m: 0.9387\n",
            "Epoch 161/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9816 - f1_m: 0.9818\n",
            "Epoch 161: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.0550 - accuracy: 0.9816 - f1_m: 0.9818 - val_loss: 0.2723 - val_accuracy: 0.9404 - val_f1_m: 0.9417\n",
            "Epoch 162/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9821 - f1_m: 0.9825\n",
            "Epoch 162: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 279ms/step - loss: 0.0535 - accuracy: 0.9821 - f1_m: 0.9825 - val_loss: 0.2830 - val_accuracy: 0.9388 - val_f1_m: 0.9403\n",
            "Epoch 163/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9845 - f1_m: 0.9848\n",
            "Epoch 163: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 282ms/step - loss: 0.0492 - accuracy: 0.9845 - f1_m: 0.9848 - val_loss: 0.2711 - val_accuracy: 0.9388 - val_f1_m: 0.9406\n",
            "Epoch 164/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9823 - f1_m: 0.9825\n",
            "Epoch 164: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 276ms/step - loss: 0.0532 - accuracy: 0.9823 - f1_m: 0.9825 - val_loss: 0.2778 - val_accuracy: 0.9388 - val_f1_m: 0.9399\n",
            "Epoch 165/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9835 - f1_m: 0.9833\n",
            "Epoch 165: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 9s 271ms/step - loss: 0.0535 - accuracy: 0.9835 - f1_m: 0.9833 - val_loss: 0.2833 - val_accuracy: 0.9380 - val_f1_m: 0.9399\n",
            "Epoch 166/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9828 - f1_m: 0.9833\n",
            "Epoch 166: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0501 - accuracy: 0.9828 - f1_m: 0.9833 - val_loss: 0.2737 - val_accuracy: 0.9384 - val_f1_m: 0.9378\n",
            "Epoch 167/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9844 - f1_m: 0.9848\n",
            "Epoch 167: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 276ms/step - loss: 0.0485 - accuracy: 0.9844 - f1_m: 0.9848 - val_loss: 0.2832 - val_accuracy: 0.9400 - val_f1_m: 0.9402\n",
            "Epoch 168/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9831 - f1_m: 0.9830\n",
            "Epoch 168: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 275ms/step - loss: 0.0522 - accuracy: 0.9831 - f1_m: 0.9830 - val_loss: 0.2687 - val_accuracy: 0.9384 - val_f1_m: 0.9403\n",
            "Epoch 169/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9816 - f1_m: 0.9820\n",
            "Epoch 169: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 275ms/step - loss: 0.0523 - accuracy: 0.9816 - f1_m: 0.9820 - val_loss: 0.2751 - val_accuracy: 0.9384 - val_f1_m: 0.9388\n",
            "Epoch 170/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9838 - f1_m: 0.9839\n",
            "Epoch 170: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 272ms/step - loss: 0.0487 - accuracy: 0.9838 - f1_m: 0.9839 - val_loss: 0.2788 - val_accuracy: 0.9364 - val_f1_m: 0.9388\n",
            "Epoch 171/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9853 - f1_m: 0.9850\n",
            "Epoch 171: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 273ms/step - loss: 0.0488 - accuracy: 0.9853 - f1_m: 0.9850 - val_loss: 0.2727 - val_accuracy: 0.9388 - val_f1_m: 0.9399\n",
            "Epoch 172/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9835 - f1_m: 0.9838\n",
            "Epoch 172: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 275ms/step - loss: 0.0489 - accuracy: 0.9835 - f1_m: 0.9838 - val_loss: 0.2770 - val_accuracy: 0.9380 - val_f1_m: 0.9382\n",
            "Epoch 173/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9839 - f1_m: 0.9839\n",
            "Epoch 173: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 273ms/step - loss: 0.0501 - accuracy: 0.9839 - f1_m: 0.9839 - val_loss: 0.2810 - val_accuracy: 0.9388 - val_f1_m: 0.9407\n",
            "Epoch 174/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9841 - f1_m: 0.9839\n",
            "Epoch 174: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 9s 268ms/step - loss: 0.0500 - accuracy: 0.9841 - f1_m: 0.9839 - val_loss: 0.2787 - val_accuracy: 0.9404 - val_f1_m: 0.9392\n",
            "Epoch 175/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9843 - f1_m: 0.9842\n",
            "Epoch 175: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0485 - accuracy: 0.9843 - f1_m: 0.9842 - val_loss: 0.2839 - val_accuracy: 0.9384 - val_f1_m: 0.9389\n",
            "Epoch 176/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9843 - f1_m: 0.9842\n",
            "Epoch 176: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 276ms/step - loss: 0.0456 - accuracy: 0.9843 - f1_m: 0.9842 - val_loss: 0.2886 - val_accuracy: 0.9400 - val_f1_m: 0.9399\n",
            "Epoch 177/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9850 - f1_m: 0.9851\n",
            "Epoch 177: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 9s 271ms/step - loss: 0.0461 - accuracy: 0.9850 - f1_m: 0.9851 - val_loss: 0.2769 - val_accuracy: 0.9376 - val_f1_m: 0.9372\n",
            "Epoch 178/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9847 - f1_m: 0.9846\n",
            "Epoch 178: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 9s 267ms/step - loss: 0.0457 - accuracy: 0.9847 - f1_m: 0.9846 - val_loss: 0.2859 - val_accuracy: 0.9384 - val_f1_m: 0.9391\n",
            "Epoch 179/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9830 - f1_m: 0.9835\n",
            "Epoch 179: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 9s 271ms/step - loss: 0.0477 - accuracy: 0.9830 - f1_m: 0.9835 - val_loss: 0.2811 - val_accuracy: 0.9396 - val_f1_m: 0.9387\n",
            "Epoch 180/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9846 - f1_m: 0.9849\n",
            "Epoch 180: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.0459 - accuracy: 0.9846 - f1_m: 0.9849 - val_loss: 0.2903 - val_accuracy: 0.9388 - val_f1_m: 0.9388\n",
            "Epoch 181/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9842 - f1_m: 0.9841\n",
            "Epoch 181: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 9s 269ms/step - loss: 0.0496 - accuracy: 0.9842 - f1_m: 0.9841 - val_loss: 0.2884 - val_accuracy: 0.9396 - val_f1_m: 0.9392\n",
            "Epoch 182/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9855 - f1_m: 0.9852\n",
            "Epoch 182: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 276ms/step - loss: 0.0457 - accuracy: 0.9855 - f1_m: 0.9852 - val_loss: 0.2781 - val_accuracy: 0.9368 - val_f1_m: 0.9375\n",
            "Epoch 183/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9859 - f1_m: 0.9861\n",
            "Epoch 183: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.0433 - accuracy: 0.9859 - f1_m: 0.9861 - val_loss: 0.2853 - val_accuracy: 0.9388 - val_f1_m: 0.9405\n",
            "Epoch 184/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9850 - f1_m: 0.9854\n",
            "Epoch 184: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 279ms/step - loss: 0.0454 - accuracy: 0.9850 - f1_m: 0.9854 - val_loss: 0.2859 - val_accuracy: 0.9404 - val_f1_m: 0.9412\n",
            "Epoch 185/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9876 - f1_m: 0.9876\n",
            "Epoch 185: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 272ms/step - loss: 0.0406 - accuracy: 0.9876 - f1_m: 0.9876 - val_loss: 0.2909 - val_accuracy: 0.9408 - val_f1_m: 0.9410\n",
            "Epoch 186/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 0.9848 - f1_m: 0.9847\n",
            "Epoch 186: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0446 - accuracy: 0.9848 - f1_m: 0.9847 - val_loss: 0.2904 - val_accuracy: 0.9360 - val_f1_m: 0.9384\n",
            "Epoch 187/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9863 - f1_m: 0.9866\n",
            "Epoch 187: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0437 - accuracy: 0.9863 - f1_m: 0.9866 - val_loss: 0.2869 - val_accuracy: 0.9388 - val_f1_m: 0.9390\n",
            "Epoch 188/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9859 - f1_m: 0.9858\n",
            "Epoch 188: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 274ms/step - loss: 0.0457 - accuracy: 0.9859 - f1_m: 0.9858 - val_loss: 0.2794 - val_accuracy: 0.9364 - val_f1_m: 0.9376\n",
            "Epoch 189/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9855 - f1_m: 0.9857\n",
            "Epoch 189: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 273ms/step - loss: 0.0434 - accuracy: 0.9855 - f1_m: 0.9857 - val_loss: 0.2869 - val_accuracy: 0.9388 - val_f1_m: 0.9398\n",
            "Epoch 190/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9844 - f1_m: 0.9847\n",
            "Epoch 190: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.0428 - accuracy: 0.9844 - f1_m: 0.9847 - val_loss: 0.2864 - val_accuracy: 0.9380 - val_f1_m: 0.9379\n",
            "Epoch 191/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9859 - f1_m: 0.9856\n",
            "Epoch 191: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 9s 268ms/step - loss: 0.0434 - accuracy: 0.9859 - f1_m: 0.9856 - val_loss: 0.2834 - val_accuracy: 0.9392 - val_f1_m: 0.9410\n",
            "Epoch 192/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9858 - f1_m: 0.9859\n",
            "Epoch 192: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0425 - accuracy: 0.9858 - f1_m: 0.9859 - val_loss: 0.2941 - val_accuracy: 0.9388 - val_f1_m: 0.9378\n",
            "Epoch 193/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9858 - f1_m: 0.9856\n",
            "Epoch 193: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 273ms/step - loss: 0.0424 - accuracy: 0.9858 - f1_m: 0.9856 - val_loss: 0.2950 - val_accuracy: 0.9396 - val_f1_m: 0.9385\n",
            "Epoch 194/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9836 - f1_m: 0.9838\n",
            "Epoch 194: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 279ms/step - loss: 0.0484 - accuracy: 0.9836 - f1_m: 0.9838 - val_loss: 0.2854 - val_accuracy: 0.9384 - val_f1_m: 0.9388\n",
            "Epoch 195/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9860 - f1_m: 0.9860\n",
            "Epoch 195: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0433 - accuracy: 0.9860 - f1_m: 0.9860 - val_loss: 0.2842 - val_accuracy: 0.9388 - val_f1_m: 0.9392\n",
            "Epoch 196/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9857 - f1_m: 0.9856\n",
            "Epoch 196: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 273ms/step - loss: 0.0426 - accuracy: 0.9857 - f1_m: 0.9856 - val_loss: 0.2836 - val_accuracy: 0.9392 - val_f1_m: 0.9403\n",
            "Epoch 197/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9871 - f1_m: 0.9870\n",
            "Epoch 197: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0404 - accuracy: 0.9871 - f1_m: 0.9870 - val_loss: 0.2834 - val_accuracy: 0.9392 - val_f1_m: 0.9403\n",
            "Epoch 198/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9863 - f1_m: 0.9863\n",
            "Epoch 198: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0421 - accuracy: 0.9863 - f1_m: 0.9863 - val_loss: 0.2888 - val_accuracy: 0.9416 - val_f1_m: 0.9411\n",
            "Epoch 199/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9858 - f1_m: 0.9860\n",
            "Epoch 199: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.0425 - accuracy: 0.9858 - f1_m: 0.9860 - val_loss: 0.2964 - val_accuracy: 0.9400 - val_f1_m: 0.9400\n",
            "Epoch 200/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9875 - f1_m: 0.9872\n",
            "Epoch 200: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0403 - accuracy: 0.9875 - f1_m: 0.9872 - val_loss: 0.2701 - val_accuracy: 0.9420 - val_f1_m: 0.9412\n",
            "Epoch 201/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9865 - f1_m: 0.9867\n",
            "Epoch 201: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 274ms/step - loss: 0.0411 - accuracy: 0.9865 - f1_m: 0.9867 - val_loss: 0.2815 - val_accuracy: 0.9420 - val_f1_m: 0.9424\n",
            "Epoch 202/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9872 - f1_m: 0.9872\n",
            "Epoch 202: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.0398 - accuracy: 0.9872 - f1_m: 0.9872 - val_loss: 0.2851 - val_accuracy: 0.9412 - val_f1_m: 0.9416\n",
            "Epoch 203/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9868 - f1_m: 0.9867\n",
            "Epoch 203: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 273ms/step - loss: 0.0404 - accuracy: 0.9868 - f1_m: 0.9867 - val_loss: 0.2914 - val_accuracy: 0.9400 - val_f1_m: 0.9406\n",
            "Epoch 204/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9883 - f1_m: 0.9881\n",
            "Epoch 204: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.0384 - accuracy: 0.9883 - f1_m: 0.9881 - val_loss: 0.2852 - val_accuracy: 0.9396 - val_f1_m: 0.9415\n",
            "Epoch 205/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9874 - f1_m: 0.9874\n",
            "Epoch 205: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 275ms/step - loss: 0.0387 - accuracy: 0.9874 - f1_m: 0.9874 - val_loss: 0.2916 - val_accuracy: 0.9400 - val_f1_m: 0.9406\n",
            "Epoch 206/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9866 - f1_m: 0.9865\n",
            "Epoch 206: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 274ms/step - loss: 0.0408 - accuracy: 0.9866 - f1_m: 0.9865 - val_loss: 0.2908 - val_accuracy: 0.9376 - val_f1_m: 0.9391\n",
            "Epoch 207/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9863 - f1_m: 0.9863\n",
            "Epoch 207: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 272ms/step - loss: 0.0409 - accuracy: 0.9863 - f1_m: 0.9863 - val_loss: 0.2931 - val_accuracy: 0.9424 - val_f1_m: 0.9415\n",
            "Epoch 208/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9876 - f1_m: 0.9877\n",
            "Epoch 208: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 273ms/step - loss: 0.0379 - accuracy: 0.9876 - f1_m: 0.9877 - val_loss: 0.2993 - val_accuracy: 0.9368 - val_f1_m: 0.9378\n",
            "Epoch 209/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9876 - f1_m: 0.9873\n",
            "Epoch 209: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 272ms/step - loss: 0.0385 - accuracy: 0.9876 - f1_m: 0.9873 - val_loss: 0.2878 - val_accuracy: 0.9388 - val_f1_m: 0.9412\n",
            "Epoch 210/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9867 - f1_m: 0.9869\n",
            "Epoch 210: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.0405 - accuracy: 0.9867 - f1_m: 0.9869 - val_loss: 0.3034 - val_accuracy: 0.9380 - val_f1_m: 0.9394\n",
            "Epoch 211/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9888 - f1_m: 0.9887\n",
            "Epoch 211: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 9s 270ms/step - loss: 0.0359 - accuracy: 0.9888 - f1_m: 0.9887 - val_loss: 0.2962 - val_accuracy: 0.9376 - val_f1_m: 0.9394\n",
            "Epoch 212/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9883 - f1_m: 0.9879\n",
            "Epoch 212: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 272ms/step - loss: 0.0370 - accuracy: 0.9883 - f1_m: 0.9879 - val_loss: 0.3032 - val_accuracy: 0.9388 - val_f1_m: 0.9389\n",
            "Epoch 213/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9873 - f1_m: 0.9875\n",
            "Epoch 213: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 278ms/step - loss: 0.0386 - accuracy: 0.9873 - f1_m: 0.9875 - val_loss: 0.3004 - val_accuracy: 0.9356 - val_f1_m: 0.9375\n",
            "Epoch 214/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9888 - f1_m: 0.9887\n",
            "Epoch 214: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0359 - accuracy: 0.9888 - f1_m: 0.9887 - val_loss: 0.2985 - val_accuracy: 0.9400 - val_f1_m: 0.9406\n",
            "Epoch 215/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9878 - f1_m: 0.9879\n",
            "Epoch 215: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0378 - accuracy: 0.9878 - f1_m: 0.9879 - val_loss: 0.2904 - val_accuracy: 0.9388 - val_f1_m: 0.9394\n",
            "Epoch 216/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9864 - f1_m: 0.9866\n",
            "Epoch 216: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 283ms/step - loss: 0.0398 - accuracy: 0.9864 - f1_m: 0.9866 - val_loss: 0.2920 - val_accuracy: 0.9400 - val_f1_m: 0.9409\n",
            "Epoch 217/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9893 - f1_m: 0.9890\n",
            "Epoch 217: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.0342 - accuracy: 0.9893 - f1_m: 0.9890 - val_loss: 0.2945 - val_accuracy: 0.9384 - val_f1_m: 0.9393\n",
            "Epoch 218/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9880 - f1_m: 0.9879\n",
            "Epoch 218: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 284ms/step - loss: 0.0369 - accuracy: 0.9880 - f1_m: 0.9879 - val_loss: 0.2942 - val_accuracy: 0.9368 - val_f1_m: 0.9366\n",
            "Epoch 219/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9882 - f1_m: 0.9884\n",
            "Epoch 219: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 276ms/step - loss: 0.0360 - accuracy: 0.9882 - f1_m: 0.9884 - val_loss: 0.2875 - val_accuracy: 0.9412 - val_f1_m: 0.9424\n",
            "Epoch 220/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9872 - f1_m: 0.9870\n",
            "Epoch 220: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.0379 - accuracy: 0.9872 - f1_m: 0.9870 - val_loss: 0.2857 - val_accuracy: 0.9412 - val_f1_m: 0.9409\n",
            "Epoch 221/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9885 - f1_m: 0.9883\n",
            "Epoch 221: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.0355 - accuracy: 0.9885 - f1_m: 0.9883 - val_loss: 0.2921 - val_accuracy: 0.9420 - val_f1_m: 0.9419\n",
            "Epoch 222/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9876 - f1_m: 0.9877\n",
            "Epoch 222: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.0353 - accuracy: 0.9876 - f1_m: 0.9877 - val_loss: 0.3048 - val_accuracy: 0.9404 - val_f1_m: 0.9398\n",
            "Epoch 223/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9882 - f1_m: 0.9883\n",
            "Epoch 223: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 282ms/step - loss: 0.0351 - accuracy: 0.9882 - f1_m: 0.9883 - val_loss: 0.3102 - val_accuracy: 0.9384 - val_f1_m: 0.9394\n",
            "Epoch 224/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9881 - f1_m: 0.9881\n",
            "Epoch 224: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 284ms/step - loss: 0.0369 - accuracy: 0.9881 - f1_m: 0.9881 - val_loss: 0.3041 - val_accuracy: 0.9372 - val_f1_m: 0.9381\n",
            "Epoch 225/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9874 - f1_m: 0.9873\n",
            "Epoch 225: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 276ms/step - loss: 0.0389 - accuracy: 0.9874 - f1_m: 0.9873 - val_loss: 0.2966 - val_accuracy: 0.9372 - val_f1_m: 0.9376\n",
            "Epoch 226/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9877 - f1_m: 0.9876\n",
            "Epoch 226: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 280ms/step - loss: 0.0378 - accuracy: 0.9877 - f1_m: 0.9876 - val_loss: 0.2920 - val_accuracy: 0.9384 - val_f1_m: 0.9397\n",
            "Epoch 227/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9876 - f1_m: 0.9877\n",
            "Epoch 227: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 279ms/step - loss: 0.0370 - accuracy: 0.9876 - f1_m: 0.9877 - val_loss: 0.2994 - val_accuracy: 0.9376 - val_f1_m: 0.9387\n",
            "Epoch 228/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9897 - f1_m: 0.9896\n",
            "Epoch 228: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.0337 - accuracy: 0.9897 - f1_m: 0.9896 - val_loss: 0.3055 - val_accuracy: 0.9376 - val_f1_m: 0.9402\n",
            "Epoch 229/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9880 - f1_m: 0.9881\n",
            "Epoch 229: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 279ms/step - loss: 0.0359 - accuracy: 0.9880 - f1_m: 0.9881 - val_loss: 0.2936 - val_accuracy: 0.9384 - val_f1_m: 0.9374\n",
            "Epoch 230/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9894 - f1_m: 0.9899\n",
            "Epoch 230: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 286ms/step - loss: 0.0337 - accuracy: 0.9894 - f1_m: 0.9899 - val_loss: 0.2997 - val_accuracy: 0.9368 - val_f1_m: 0.9362\n",
            "Epoch 231/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9873 - f1_m: 0.9872\n",
            "Epoch 231: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 280ms/step - loss: 0.0382 - accuracy: 0.9873 - f1_m: 0.9872 - val_loss: 0.3010 - val_accuracy: 0.9392 - val_f1_m: 0.9390\n",
            "Epoch 232/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9879 - f1_m: 0.9877\n",
            "Epoch 232: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 280ms/step - loss: 0.0344 - accuracy: 0.9879 - f1_m: 0.9877 - val_loss: 0.2962 - val_accuracy: 0.9400 - val_f1_m: 0.9399\n",
            "Epoch 233/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9881 - f1_m: 0.9880\n",
            "Epoch 233: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 279ms/step - loss: 0.0371 - accuracy: 0.9881 - f1_m: 0.9880 - val_loss: 0.3004 - val_accuracy: 0.9408 - val_f1_m: 0.9404\n",
            "Epoch 234/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9877 - f1_m: 0.9875\n",
            "Epoch 234: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 281ms/step - loss: 0.0348 - accuracy: 0.9877 - f1_m: 0.9875 - val_loss: 0.3092 - val_accuracy: 0.9364 - val_f1_m: 0.9380\n",
            "Epoch 235/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9893 - f1_m: 0.9893\n",
            "Epoch 235: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0328 - accuracy: 0.9893 - f1_m: 0.9893 - val_loss: 0.2963 - val_accuracy: 0.9404 - val_f1_m: 0.9399\n",
            "Epoch 236/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9885 - f1_m: 0.9886\n",
            "Epoch 236: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 280ms/step - loss: 0.0344 - accuracy: 0.9885 - f1_m: 0.9886 - val_loss: 0.3016 - val_accuracy: 0.9392 - val_f1_m: 0.9398\n",
            "Epoch 237/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9896 - f1_m: 0.9899\n",
            "Epoch 237: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 287ms/step - loss: 0.0331 - accuracy: 0.9896 - f1_m: 0.9899 - val_loss: 0.3008 - val_accuracy: 0.9368 - val_f1_m: 0.9388\n",
            "Epoch 238/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9894 - f1_m: 0.9894\n",
            "Epoch 238: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 287ms/step - loss: 0.0311 - accuracy: 0.9894 - f1_m: 0.9894 - val_loss: 0.3137 - val_accuracy: 0.9388 - val_f1_m: 0.9394\n",
            "Epoch 239/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9878 - f1_m: 0.9878\n",
            "Epoch 239: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 282ms/step - loss: 0.0349 - accuracy: 0.9878 - f1_m: 0.9878 - val_loss: 0.3095 - val_accuracy: 0.9380 - val_f1_m: 0.9386\n",
            "Epoch 240/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9899 - f1_m: 0.9897\n",
            "Epoch 240: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 288ms/step - loss: 0.0319 - accuracy: 0.9899 - f1_m: 0.9897 - val_loss: 0.3230 - val_accuracy: 0.9356 - val_f1_m: 0.9370\n",
            "Epoch 241/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9889 - f1_m: 0.9889\n",
            "Epoch 241: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 285ms/step - loss: 0.0337 - accuracy: 0.9889 - f1_m: 0.9889 - val_loss: 0.2965 - val_accuracy: 0.9392 - val_f1_m: 0.9409\n",
            "Epoch 242/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9897 - f1_m: 0.9895\n",
            "Epoch 242: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 277ms/step - loss: 0.0332 - accuracy: 0.9897 - f1_m: 0.9895 - val_loss: 0.2973 - val_accuracy: 0.9380 - val_f1_m: 0.9407\n",
            "Epoch 243/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9877 - f1_m: 0.9881\n",
            "Epoch 243: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 282ms/step - loss: 0.0352 - accuracy: 0.9877 - f1_m: 0.9881 - val_loss: 0.3031 - val_accuracy: 0.9392 - val_f1_m: 0.9394\n",
            "Epoch 244/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9899 - f1_m: 0.9897\n",
            "Epoch 244: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 282ms/step - loss: 0.0303 - accuracy: 0.9899 - f1_m: 0.9897 - val_loss: 0.3149 - val_accuracy: 0.9380 - val_f1_m: 0.9395\n",
            "Epoch 245/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9885 - f1_m: 0.9882\n",
            "Epoch 245: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 284ms/step - loss: 0.0351 - accuracy: 0.9885 - f1_m: 0.9882 - val_loss: 0.3144 - val_accuracy: 0.9400 - val_f1_m: 0.9385\n",
            "Epoch 246/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9896 - f1_m: 0.9895\n",
            "Epoch 246: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 282ms/step - loss: 0.0307 - accuracy: 0.9896 - f1_m: 0.9895 - val_loss: 0.3154 - val_accuracy: 0.9384 - val_f1_m: 0.9387\n",
            "Epoch 247/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9887 - f1_m: 0.9887\n",
            "Epoch 247: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 289ms/step - loss: 0.0329 - accuracy: 0.9887 - f1_m: 0.9887 - val_loss: 0.3108 - val_accuracy: 0.9388 - val_f1_m: 0.9387\n",
            "Epoch 248/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9898 - f1_m: 0.9896\n",
            "Epoch 248: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 279ms/step - loss: 0.0306 - accuracy: 0.9898 - f1_m: 0.9896 - val_loss: 0.3073 - val_accuracy: 0.9376 - val_f1_m: 0.9385\n",
            "Epoch 249/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9891 - f1_m: 0.9892\n",
            "Epoch 249: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 284ms/step - loss: 0.0321 - accuracy: 0.9891 - f1_m: 0.9892 - val_loss: 0.3098 - val_accuracy: 0.9380 - val_f1_m: 0.9391\n",
            "Epoch 250/250\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9889 - f1_m: 0.9888\n",
            "Epoch 250: val_loss did not improve from 0.24602\n",
            "35/35 [==============================] - 10s 283ms/step - loss: 0.0346 - accuracy: 0.9889 - f1_m: 0.9888 - val_loss: 0.3021 - val_accuracy: 0.9384 - val_f1_m: 0.9386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final testing of the public test set\n",
        "\n",
        "The public test set is a sub sample of the data the model is going to see during the phase of post-deployment. The test set here is mentioned in the `test_public.csv` file. The file has a lot of ambiguities as the data might have unseen values to be classified into a rejection class.\n",
        "\n",
        "The `test` dataset need to be used here for obtaining the results."
      ],
      "metadata": {
        "id": "tlLN1p_xHBKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_final = mappingTest(test)\n",
        "import numpy as np\n",
        "X_test = []\n",
        "\n",
        "for lst in test_final:\n",
        "  X_test.append(np.array(lst, dtype=np.int32))"
      ],
      "metadata": {
        "id": "Hl_DYtaeJJCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "X_TEST = pad_sequences(maxlen=max_len, sequences=X_test, padding=\"post\", value=len(words)-1)"
      ],
      "metadata": {
        "id": "nq7CSc2ZKTHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "with open('test_public_res.csv', 'w', newline='') as file:\n",
        "  writer = csv.writer(file)\n",
        "  writer.writerow(['id', 'Predicted'])\n",
        "  \n",
        "  cnt = 0\n",
        "  idx = 0\n",
        "  for i in X_TEST:\n",
        "    p = model.predict(np.array([i]))\n",
        "    p = np.argmax(p, axis=-1)\n",
        "    count = 0\n",
        "    for val in range(len(test[cnt])):\n",
        "      if test_final[cnt][val] == len(words)-1:\n",
        "        writer.writerow([idx,''])\n",
        "      else:    \n",
        "        writer.writerow([idx,classes[p[0][count]]])\n",
        "      count +=1\n",
        "      idx += 1\n",
        "    cnt += 1\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sixMEcRm0VDC",
        "outputId": "64a4f847-8e93-49dc-ebd9-6710da7ac30d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_new = new_model = tf.keras.models.load_model('model_best.h5', custom_objects={\"f1_m\": f1_m})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRIVJzyTY-84",
        "outputId": "f8121a55-b690-45fd-b25f-4224a7841904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_final = mappingTest(test)\n",
        "import numpy as np\n",
        "X_test = []\n",
        "\n",
        "for lst in test_final:\n",
        "  X_test.append(np.array(lst, dtype=np.int32))\n",
        "\n",
        "import csv\n",
        "with open('/content/drive/MyDrive/models/prometeo/test_public_res_new.csv', 'w', newline='') as file:\n",
        "  writer = csv.writer(file)\n",
        "  writer.writerow(['id', 'Predicted'])\n",
        "  \n",
        "  cnt = 0\n",
        "  idx = 0\n",
        "  for i in X_TEST:\n",
        "    p = model_new.predict(np.array([i]))\n",
        "    p = np.argmax(p, axis=-1)\n",
        "    count = 0\n",
        "    for val in range(len(test[cnt])):\n",
        "      if test_final[cnt][val] == len(words)-1:\n",
        "        writer.writerow([idx,''])\n",
        "      else:    \n",
        "        writer.writerow([idx,classes[p[0][count]]])\n",
        "      count +=1\n",
        "      idx += 1\n",
        "    cnt += 1\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hq5LKx0EZd9l",
        "outputId": "7c34677a-32b1-45e5-b305-0e0c8c64f712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "\n",
        "\n",
        "*   Epochs and Learning rate has a considerable effect on the trained model.\n",
        "*   Model in `tensorflow` worked better than models in `pytorch`, as I am having a better familiarty with tesorflow.\n",
        "*   Epochs tested upon `64: 0.80981`, `150: 0.81682`\n",
        "\n"
      ],
      "metadata": {
        "id": "uZlTUj5ahaCw"
      }
    }
  ]
}